"""
## Dynamic harvest and transform DAG

Iterates through config files to dynamically build harvest and transform tasks
"""

from datetime import datetime, timedelta
import sys
import os

from airflow import DAG

from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.decorators import task
from airflow.models import Variable
from airflow.models.baseoperator import chain

PATH = os.path.abspath(os.path.dirname(__file__))
SSDN_ENV = Variable.get('ssdn_env')

# Import local module
sys.path.insert(0, PATH)
import ssdn_assets


with DAG('ssdn_dynamic_harvest',
         default_args={'depends_on_past': False,
                       'email': ['airflow.example.org'],
                       'email_on_failure': False,
                       'email_on_retry': False,
                       'retries': 1,
                       'retry_delay': timedelta(minutes=5),
                       'env': {'MANATUS_CONFIG': SSDN_ENV}
                       },
         description='Dynamic harvest test',
         tags=['ssdn', 'harvest', 'transform', 'dynamic', ],
         start_date=datetime(2022, 1, 1),
         schedule_interval='@quarterly',
         catchup=False,
         doc_md=__doc__,
         ) as dag:

    # Update ssdn config and maps git repos
    repo_update = BashOperator(
        task_id='repo_update',
        bash_command=f'bash {PATH}/ssdn_assets/repo_update.sh {Variable.get("ssdn_git_repos")}',
    )

    # Clean up OAI data path
    clean_up = BashOperator(
        task_id='clean_up',
        bash_command=f'rm -rf {ssdn_assets.OAI_PATH}/*/*.xml',
    )

    @task(task_id='add_flmem')
    def add_flmem():
        """Add rolling Florida Memory JSON"""
        ssdn_assets.add_json(Variable.get("flmem_data"), ssdn_assets.JSONL_PATH)
    add_data = add_flmem()

    # @task(task_id='dedupe_records')
    # def dedupe():
    #     """Dedupe records"""
    #     # TODO

    @task(task_id='dpla_local_subjects')
    def dpla_local_subjects():
        """
        Add SSDN subject maps: https://github.com/mrmiguez/dpla_local_subjects/blob/master/dpla_local_map/__init__.py
        """
        ssdn_assets.dpla_local_subjects(ssdn_assets.JSONL_PATH)
        # attach a logger?
        print(f"Calling: dpla_local_subjects 1")
    dpla_local_subjects = dpla_local_subjects()

    @task(task_id='count_records')
    def count_records():
        """Count and print records by provider"""
        ssdn_assets.count_records(ssdn_assets.JSONL_PATH)
    count_records = count_records()

    for partner in ssdn_assets.list_config_keys(ssdn_assets.harvest_parser):
        partner_harvest = BashOperator(
            task_id=f'harvest_{partner}',
            bash_command=f'python3 -m manatus --profile ssdn harvest -s {partner}',
            doc_md="""\
            ### Transformation tasks

            Dynamically generated by iterating through configs
            """,
        )

        partner_transform = BashOperator(
            task_id=f'transform_{partner}',
            bash_command=f'python3 -m manatus --profile ssdn transform -s {partner}',
            doc_md="""\
            ### Transformation tasks
            
            Dynamically generated by iterating through configs
            """,
        )

        chain([repo_update, clean_up], partner_harvest, partner_transform, add_data, count_records)
        # TODO: add dedupe and subject tasks to chain
